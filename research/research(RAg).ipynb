{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e9e3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_google_genai  import GoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539a3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8470421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(groq_api_key=groq_api_key,\n",
    "             model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da096acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"C:\\\\QNARAG\\\\IEEE_SMC_2025_Omar.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a643d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3b034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "final_documents = text_splitter.split_documents(doc[:20])  # splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3820524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\QNARAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cffd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba37ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(final_documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1cb051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are an AI research assistant with expertise in analyzing academic papers. \n",
    "Answer the user's question based only on the provided context.\n",
    "If the answer is not found in the context, state that clearly.\n",
    "Context: {context}\n",
    "Questions:{input}\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51f67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fceb2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver=vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1444a21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time : 0.0625\n",
      "The provided context does not contain a summary of a paper. It appears to be a collection of references and a discussion of the performance of various deep learning models, including ResNet, Xception, and MobileNetV2, on a specific task, possibly related to image segmentation or object detection.\n",
      "\n",
      "The text describes the performance of these models in terms of their Intersection over Union (IoU) scores, which is a common metric used to evaluate the performance of models in image segmentation tasks. The discussion highlights the strengths and weaknesses of each model, including their ability to generalize, converge, and trade off accuracy for efficiency.\n",
      "\n",
      "However, without a clear summary of the paper, it is difficult to provide a detailed explanation of the paper's main contributions, methodology, or conclusions. If you could provide more context or clarify which specific paper you would like me to summarize, I would be happy to try and assist you further.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "retriever=vectorstore.as_retriever()\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "start=time.process_time()\n",
    "response=retrieval_chain.invoke({'input':\"Explain the summary of the paper in detail.\",\n",
    "                                 'context':\" \".join([doc.page_content for doc in final_documents[:3]])})\n",
    "print(\"Response time :\",time.process_time()-start)\n",
    "#print(response['answer'])\n",
    "if isinstance(response['answer'], list):  \n",
    "    print(response['answer'][0])  # take the first response\n",
    "else:\n",
    "    print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41f80450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context appears to be a portion of a research paper discussing the performance of various deep learning models on a specific task, likely related to computer vision and scene understanding for autonomous vehicles. Here's a detailed summary based on the given context:\n",
      "\n",
      "1. **Data Preprocessing**: The paper mentions a preprocessing pipeline where images are normalized to a [0, 1] range, and masks retain their original color structure. A consistent seed is used to ensure proper alignment between images and masks. This step is crucial for preparing the dataset for training and validation.\n",
      "\n",
      "2. **Model Performance Comparison**: The paper compares the performance of several deep learning models, including ResNet-50, Xception, MobileNetV2, ResNet18, and InceptionResNetV2. The performance is evaluated based on the Intersection over Union (IoU) metric, which measures the accuracy of the model in segmenting objects or scenes.\n",
      "\n",
      "3. **Training and Validation IoU**: The results show that InceptionResNetV2 and Xception achieve high IoU with minimal fluctuations, indicating strong generalization and robust feature learning. ResNet18 and ResNet50 exhibit moderate performance, with IoU improving steadily but showcasing larger validation fluctuations in early epochs. MobileNetV2, on the other hand, shows a relatively lower IoU compared to more complex backbones, with noticeable differences between training and validation IoU, reflecting its reduced capacity to generalize effectively.\n",
      "\n",
      "4. **Trade-off between Accuracy and Efficiency**: The findings highlight the trade-off between model accuracy and efficiency. InceptionResNetV2 and Xception are the top-performing models, but they may require more computational resources. MobileNetV2, while less accurate, maintains a lightweight architecture, making it more efficient.\n",
      "\n",
      "5. **Related Work**: The paper cites several references, including research on deep residual networks, panoptic driving perception, road scene segmentation, and explainable AI in scene understanding for autonomous vehicles. These references suggest that the paper is part of a broader research area focused on developing accurate and efficient models for autonomous vehicle applications.\n",
      "\n",
      "In summary, the paper presents a comparison of various deep learning models for a computer vision task, highlighting their strengths and weaknesses in terms of accuracy and efficiency. The findings can inform the development of more effective and efficient models for autonomous vehicle applications. However, without more context, it is unclear what specific task the models are being evaluated on or what the broader implications of the research are."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# The .stream() method returns a generator that yields dictionary chunks.\n",
    "# We iterate through the generator to get the response as it's created.\n",
    "for chunk in retrieval_chain.stream({'input': \"Explain the summary of the paper in detail.\"}):\n",
    "    # The actual generated text is usually in the 'answer' key of the chunk.\n",
    "    if \"answer\" in chunk:\n",
    "        # Print the chunk of the answer without a newline, and flush the output\n",
    "        # to ensure it appears immediately in the console.\n",
    "        print(chunk['answer'], end=\"\", flush=True)\n",
    "\n",
    "#   print(\"\\n\\nResponse time :\", time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1204593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "# import os\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# # --- Configuration ---\n",
    "# # Make sure to set your Groq API key in your environment variables\n",
    "# # For example: os.environ[\"GROQ_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "# # You can get a key from the Groq console.\n",
    "\n",
    "# PDF_PATH = \"Human_Segmentation_Research.pdf\"  # IMPORTANT: Place your PDF in the same directory\n",
    "\n",
    "# # --- 1. Text Extraction from PDF ---\n",
    "# # Note: Image extraction has been removed as Groq models are not multimodal.\n",
    "\n",
    "# def extract_pdf_text(pdf_path):\n",
    "#     \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "#     if not os.path.exists(pdf_path):\n",
    "#         print(f\"Error: PDF file not found at {pdf_path}\")\n",
    "#         # Create a dummy file to avoid crashing the rest of the script\n",
    "#         with open(pdf_path, \"w\") as f:\n",
    "#             f.write(\"Dummy PDF content. Please replace with your actual PDF.\")\n",
    "    \n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     text_content = \"\"\n",
    "#     for page_num in range(len(doc)):\n",
    "#         page = doc.load_page(page_num)\n",
    "#         text_content += page.get_text()\n",
    "            \n",
    "#     return text_content\n",
    "\n",
    "# # --- 2. LLM Invocation with Groq ---\n",
    "\n",
    "# def explain_document(pdf_path, question):\n",
    "#     \"\"\"\n",
    "#     Extracts text from a PDF and uses the Groq Llama 3 model to answer a question.\n",
    "#     \"\"\"\n",
    "#     print(f\"Analyzing text from '{pdf_path}'...\")\n",
    "#     text_content = extract_pdf_text(pdf_path)\n",
    "\n",
    "#     # Initialize the Groq model.\n",
    "#     # We are using Llama 3, which is a powerful text-based model.\n",
    "#     # It cannot process images.\n",
    "#     try:\n",
    "#         llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error initializing Groq LLM. Have you set your GROQ_API_KEY? Error: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # A prompt template designed for text-based analysis.\n",
    "#     prompt_template = ChatPromptTemplate.from_template(\n",
    "#         \"You are an AI research assistant. Based on the following context from a research paper, \"\n",
    "#         \"please answer this question: {question}\\n\\n\"\n",
    "#         \"--- Text Context ---\\n{context}\"\n",
    "#     )\n",
    "\n",
    "#     # Create a simple chain to pipe the prompt and the LLM.\n",
    "#     chain = prompt_template | llm\n",
    "\n",
    "#     # --- 3. Stream the response ---\n",
    "#     print(\"\\n--- Generating Explanation (Streaming) ---\")\n",
    "#     full_response = \"\"\n",
    "#     # We use a limited context window to fit within the model's limits.\n",
    "#     for chunk in chain.stream({\"question\": question, \"context\": text_content[:8000]}):\n",
    "#         # The actual generated text is in the 'content' attribute of the chunk.\n",
    "#         print(chunk.content, end=\"\", flush=True)\n",
    "#         full_response += chunk.content\n",
    "        \n",
    "#     return full_response\n",
    "\n",
    "\n",
    "# # --- Example Usage ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # The question is now focused on text, not graphs.\n",
    "#     user_question = \"Author details of this paper.\"\n",
    "    \n",
    "#     if not os.path.exists(PDF_PATH):\n",
    "#          print(\"\\n\\n-------------------------------------------------------------\")\n",
    "#          print(f\"WARNING: The file '{PDF_PATH}' was not found.\")\n",
    "#          print(\"Please add the PDF to your project directory to run this example.\")\n",
    "#          print(\"-------------------------------------------------------------\")\n",
    "#     else:\n",
    "#         explanation = explain_document(PDF_PATH, user_question)\n",
    "#         print(\"\\n\\n--- Analysis Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
